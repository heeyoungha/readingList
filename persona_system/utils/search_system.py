"""
ğŸ” Phase 3: ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„

ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ, ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”© ë³€í™˜, FAISS ìœ ì‚¬ë„ ê²€ìƒ‰, ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
"""
import os
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime
from loguru import logger

from .embedding_generator import EmbeddingGenerator
from .vector_database import VectorDatabase
from .text_preprocessor import TextPreprocessor


class SearchSystem:
    """Phase 3: ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 vector_db_path: str = "./data/faiss_index",
                 embedding_model: str = "klue/roberta-base"):
        """
        ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            vector_db_path: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.vector_db_path = vector_db_path
        self.embedding_model = embedding_model
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.embedding_generator = EmbeddingGenerator(model_name=embedding_model)
        self.vector_db = VectorDatabase(index_path=vector_db_path)
        self.text_preprocessor = TextPreprocessor()
        
        # ê²€ìƒ‰ ì„¤ì •
        self.default_k = 5  # ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        self.min_similarity_threshold = 0.3  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
        
        logger.info("ğŸ” Phase 3 ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ë²¡í„° DB ê²½ë¡œ: {vector_db_path}")
        logger.info(f"  - ì„ë² ë”© ëª¨ë¸: {embedding_model}")
    
    def load_index(self, index_path: Optional[str] = None) -> bool:
        """
        ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        
        Args:
            index_path: ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.vector_db.load_index(index_path)
            logger.info("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def preprocess_query(self, query: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ ì „ì²˜ë¦¬
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ ì§ˆë¬¸
        """
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        processed_query = self.text_preprocessor.clean_text(query)
        
        # ì§ˆë¬¸ í™•ì¥ (ì„ íƒì )
        # ì˜ˆ: "ì±… ì¶”ì²œí•´ì¤˜" -> "ì±… ì¶”ì²œ ë„ì„œ ë…ì„œ ë¬¸í•™"
        expanded_query = self._expand_query(processed_query)
        
        logger.info(f"ì§ˆë¬¸ ì „ì²˜ë¦¬: '{query}' -> '{expanded_query}'")
        return expanded_query
    
    def _expand_query(self, query: str) -> str:
        """
        ì§ˆë¬¸ í™•ì¥ (ë™ì˜ì–´, ê´€ë ¨ì–´ ì¶”ê°€)
        
        Args:
            query: ì „ì²˜ë¦¬ëœ ì§ˆë¬¸
            
        Returns:
            í™•ì¥ëœ ì§ˆë¬¸
        """
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ í™•ì¥ ê·œì¹™
        expansion_rules = {
            "ì±…": "ë„ì„œ ë¬¸í•™ ë…ì„œ",
            "ì¶”ì²œ": "ì œì•ˆ ê¶Œì¥ ì†Œê°œ",
            "ê°ì •": "ê¸°ë¶„ ëŠë‚Œ ì •ì„œ",
            "í”„ë¡œì íŠ¸": "ê³¼ì œ ì‘ì—… ê³„íš",
            "ì•„ì´ë””ì–´": "ìƒê° ë°œìƒ ì°½ì˜",
            "ê¸€ì“°ê¸°": "ì‘ë¬¸ ë¬¸ì„œ ì‘ì„±"
        }
        
        expanded_terms = []
        for keyword, synonyms in expansion_rules.items():
            if keyword in query:
                expanded_terms.extend(synonyms.split())
        
        if expanded_terms:
            return f"{query} {' '.join(expanded_terms)}"
        return query
    
    def generate_query_embedding(self, query: str) -> np.ndarray:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ì§ˆë¬¸ ì„ë² ë”© ë²¡í„°
        """
        try:
            # ì§ˆë¬¸ ì „ì²˜ë¦¬
            processed_query = self.preprocess_query(query)
            
            # ì„ë² ë”© ìƒì„±
            embedding_result = self.embedding_generator.generate_embeddings([processed_query])
            
            # ê²°ê³¼ê°€ dict í˜•íƒœì¸ì§€ numpy ë°°ì—´ì¸ì§€ í™•ì¸
            if isinstance(embedding_result, dict):
                embedding = embedding_result['embeddings'][0]
            else:
                embedding = embedding_result[0]
            
            logger.info(f"ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embedding.shape}")
            return embedding
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def semantic_search(self, 
                       query: str, 
                       k: int = None,
                       similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """
        ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if k is None:
            k = self.default_k
        if similarity_threshold is None:
            similarity_threshold = self.min_similarity_threshold
        
        logger.info(f"ğŸ” ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œì‘: '{query}'")
        logger.info(f"  - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {k}")
        logger.info(f"  - ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
        
        try:
            # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_query_embedding(query)
            
            # 2. FAISS ìœ ì‚¬ë„ ê²€ìƒ‰
            distances, indices, metadata = self.vector_db.search(query_embedding, k)
            
            # 3. ê²°ê³¼ í•„í„°ë§ ë° ì •ë¦¬
            filtered_results = []
            for i, meta in enumerate(metadata):
                similarity_score = float(distances[i])
                
                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì²´í¬
                if similarity_score >= similarity_threshold:
                    result = {
                        **meta,
                        'similarity_score': similarity_score,
                        'rank': i + 1,
                        'query': query
                    }
                    filtered_results.append(result)
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼ (ì„ê³„ê°’ {similarity_threshold} ì´ìƒ)")
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def batch_search(self, 
                    queries: List[str], 
                    k: int = None) -> List[List[Dict[str, Any]]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë°°ì¹˜ ê²€ìƒ‰
        
        Args:
            queries: ê²€ìƒ‰ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            k: ê° ì§ˆë¬¸ë‹¹ ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê° ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if k is None:
            k = self.default_k
        
        logger.info(f"ğŸ” ë°°ì¹˜ ê²€ìƒ‰ ì‹œì‘: {len(queries)}ê°œ ì§ˆë¬¸")
        
        try:
            # 1. ëª¨ë“  ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            query_embeddings = []
            for query in queries:
                embedding = self.generate_query_embedding(query)
                query_embeddings.append(embedding)
            
            query_embeddings = np.array(query_embeddings)
            
            # 2. ë°°ì¹˜ ê²€ìƒ‰ ì‹¤í–‰
            distances, indices, all_metadata = self.vector_db.batch_search(query_embeddings, k)
            
            # 3. ê²°ê³¼ ì •ë¦¬
            all_results = []
            for i, (query, query_metadata) in enumerate(zip(queries, all_metadata)):
                query_results = []
                for j, meta in enumerate(query_metadata):
                    result = {
                        **meta,
                        'query': query,
                        'batch_index': i
                    }
                    query_results.append(result)
                all_results.append(query_results)
            
            logger.info(f"âœ… ë°°ì¹˜ ê²€ìƒ‰ ì™„ë£Œ: {len(queries)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬")
            
            return all_results
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return [[] for _ in queries]
    
    def compose_search_context(self, 
                              search_results: List[Dict[str, Any]],
                              max_context_length: int = 2000) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        if not search_results:
            return {
                'context_text': "",
                'source_count': 0,
                'total_length': 0,
                'sources': []
            }
        
        logger.info(f"ğŸ“ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: {len(search_results)}ê°œ ê²°ê³¼")
        
        # 1. ê²°ê³¼ë¥¼ ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(search_results, 
                              key=lambda x: x.get('similarity_score', 0), 
                              reverse=True)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        current_length = 0
        used_sources = []
        
        for i, result in enumerate(sorted_results):
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            content = result.get('content', result.get('text', ''))
            if not content:
                continue
            
            # ê¸¸ì´ ì²´í¬
            if current_length + len(content) > max_context_length:
                # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìë¥´ê¸°
                remaining_space = max_context_length - current_length
                if remaining_space > 100:  # ìµœì†Œ 100ìëŠ” ë˜ì–´ì•¼ ì˜ë¯¸ê°€ ìˆìŒ
                    content = content[:remaining_space-3] + "..."
                else:
                    break
            
            # ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            context_parts.append(f"[ì¶œì²˜ {i+1}] {content}")
            current_length += len(content)
            
            # ì¶œì²˜ ì •ë³´ ì €ì¥
            source_info = {
                'rank': i + 1,
                'similarity_score': result.get('similarity_score', 0),
                'source_type': result.get('type', 'unknown'),
                'metadata': {k: v for k, v in result.items() 
                           if k not in ['content', 'text']}
            }
            used_sources.append(source_info)
        
        # 3. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = "\n\n".join(context_parts)
        
        context_info = {
            'context_text': context_text,
            'source_count': len(used_sources),
            'total_length': len(context_text),
            'sources': used_sources,
            'truncated': current_length >= max_context_length
        }
        
        logger.info(f"âœ… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ:")
        logger.info(f"  - ì‚¬ìš©ëœ ì¶œì²˜: {len(used_sources)}ê°œ")
        logger.info(f"  - ì´ ê¸¸ì´: {len(context_text)}ì")
        logger.info(f"  - ì˜ë¦¼ ì—¬ë¶€: {context_info['truncated']}")
        
        return context_info
    
    def advanced_search(self, 
                       query: str,
                       search_type: str = "semantic",
                       filters: Optional[Dict[str, Any]] = None,
                       k: int = None) -> Dict[str, Any]:
        """
        ê³ ê¸‰ ê²€ìƒ‰ (í•„í„°ë§, ë‹¤ì–‘í•œ ê²€ìƒ‰ íƒ€ì… ì§€ì›)
        
        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            search_type: ê²€ìƒ‰ íƒ€ì… ("semantic", "keyword", "hybrid")
            filters: í•„í„° ì¡°ê±´
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ì™€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        if k is None:
            k = self.default_k
        
        logger.info(f"ğŸ” ê³ ê¸‰ ê²€ìƒ‰: '{query}' (íƒ€ì…: {search_type})")
        
        try:
            # 1. ê¸°ë³¸ ì˜ë¯¸ì  ê²€ìƒ‰
            search_results = self.semantic_search(query, k * 2)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            
            # 2. í•„í„° ì ìš©
            if filters:
                search_results = self._apply_filters(search_results, filters)
            
            # 3. ê²°ê³¼ ìˆ˜ ì¡°ì •
            search_results = search_results[:k]
            
            # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_info = self.compose_search_context(search_results)
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'query': query,
                'search_type': search_type,
                'results': search_results,
                'context': context_info,
                'metadata': {
                    'total_results': len(search_results),
                    'search_timestamp': datetime.now().isoformat(),
                    'filters_applied': filters is not None
                }
            }
            
            logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                'query': query,
                'search_type': search_type,
                'results': [],
                'context': {'context_text': "", 'source_count': 0},
                'error': str(e)
            }
    
    def _apply_filters(self, 
                      results: List[Dict[str, Any]], 
                      filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ì— í•„í„° ì ìš©
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            filters: í•„í„° ì¡°ê±´
            
        Returns:
            í•„í„°ë§ëœ ê²°ê³¼
        """
        filtered_results = []
        
        for result in results:
            include_result = True
            
            # íƒ€ì… í•„í„°
            if 'type' in filters:
                if result.get('type') != filters['type']:
                    include_result = False
            
            # ë‚ ì§œ ë²”ìœ„ í•„í„°
            if 'date_range' in filters and include_result:
                result_date = result.get('date')
                if result_date:
                    # ë‚ ì§œ ë²”ìœ„ ì²´í¬ ë¡œì§ (êµ¬í˜„ í•„ìš”)
                    pass
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°
            if 'min_similarity' in filters and include_result:
                if result.get('similarity_score', 0) < filters['min_similarity']:
                    include_result = False
            
            if include_result:
                filtered_results.append(result)
        
        logger.info(f"í•„í„° ì ìš©: {len(results)} -> {len(filtered_results)}ê°œ ê²°ê³¼")
        
        return filtered_results
    
    def get_search_stats(self) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì‹œìŠ¤í…œ í†µê³„ ì •ë³´
        
        Returns:
            í†µê³„ ì •ë³´
        """
        vector_stats = self.vector_db.get_index_stats()
        
        stats = {
            'system_status': 'ready' if vector_stats['status'] == 'ready' else 'not_ready',
            'vector_database': vector_stats,
            'embedding_model': self.embedding_model,
            'default_k': self.default_k,
            'similarity_threshold': self.min_similarity_threshold,
            'components': {
                'embedding_generator': 'loaded' if self.embedding_generator.model else 'not_loaded',
                'vector_database': vector_stats['status'],
                'text_preprocessor': 'ready'
            }
        }
        
        return stats 